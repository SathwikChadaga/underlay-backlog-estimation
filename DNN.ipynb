{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11cf7bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import utils.network as network_class\n",
    "# import utils.lstm_model as lstm_model_class\n",
    "import utils.model_trainer as model_trainer_class\n",
    "import utils.data_processer as data_processer_functions\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fl\n",
    "\n",
    "from importlib import reload\n",
    "from tqdm import trange\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bffe637d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2080 Ti\n",
      "CUDA Version 12.1\n",
      "PyTorch Version 2.1.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "print('CUDA Version', torch.version.cuda)\n",
    "print('PyTorch Version', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c87f3",
   "metadata": {},
   "source": [
    "# Defining the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03b55891",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 5\n",
    "num_edges = 5\n",
    "\n",
    "adjacency_nodexedge = np.zeros([num_nodes, num_edges]) # node x edge adjacency matrix of the network (with all tunnels)\n",
    "adjacency_nodexedge = np.array([[-1,0,0,0,0], # -1 => node is edge's source\n",
    "                                [1,-1,-1,0,0], # 1 => node is edge's destination\n",
    "                                [0,1,0,-1,0],\n",
    "                                [0,0,1,1,-1],\n",
    "                                [0,0,0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8110eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tunnel level parameters\n",
    "num_tunnels = 2\n",
    "\n",
    "is_edge_in_tunnel      = np.zeros([num_tunnels, num_edges, ])\n",
    "external_arrival_rates = np.zeros([num_tunnels, num_nodes, ])\n",
    "overlay_service_rates  = np.zeros([num_tunnels, num_edges, ])\n",
    "underlay_service_rates = np.zeros([num_tunnels, num_edges, ])\n",
    "\n",
    "# tunnel 0\n",
    "is_edge_in_tunnel[0,:]      = np.array([1,1,0,1,1]) # 1 => edge is a part of the tunnel, 0 => otherwise\n",
    "external_arrival_rates[0,:] = np.array([0.9,0,0,0,0])\n",
    "overlay_service_rates[0,:]  = np.array([1,0,0,0,0])\n",
    "underlay_service_rates[0,:] = np.array([0,1,0,1,1])\n",
    "\n",
    "# tunnel 1\n",
    "is_edge_in_tunnel[1,:]      = np.array([1,0,1,0,1]) # 1 => edge is a part of the tunnel, 0 => otherwise\n",
    "external_arrival_rates[1,:] = np.array([0.9,0,0,0,0])\n",
    "overlay_service_rates[1,:]  = np.array([1,0,0,0,0])\n",
    "underlay_service_rates[1,:] = np.array([0,0,1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "362cee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get network class\n",
    "reload(network_class)\n",
    "my_network = network_class.Network(\n",
    "    num_tunnels,\n",
    "    num_nodes,\n",
    "    num_edges,\n",
    "    adjacency_nodexedge,\n",
    "    is_edge_in_tunnel, \n",
    "    underlay_service_rates,\n",
    "    external_arrival_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3321ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate network\n",
    "packets_in_flight, tunnel_backlogs = my_network.simulate(overlay_service_rates, total_time = 100000, custom_seed = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aef40c",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "254df219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class DNN_pool(nn.Module):\n",
    "    def __init__(self,in_size,hid_sizes,out_size):\n",
    "        super(DNN_pool, self).__init__()\n",
    "        self.input_size=in_size\n",
    "        self.output_size=out_size\n",
    "        self.hidden_sizes=hid_sizes\n",
    "        print('hidden sizes is :',hid_sizes)\n",
    "        \n",
    "        # Fully connected hidden layers\n",
    "        self.input_layer = nn.Linear(in_size, hid_sizes[0])  \n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hid_sizes[i], hid_sizes[i+1]) \n",
    "                                            for i in range(len(hid_sizes) - 1)])\n",
    "        \n",
    "#         self.fc2 = nn.Linear(128, 64)\n",
    "#         self.fc3 = nn.Linear(64, out_size)\n",
    "\n",
    "        # Max pooling layer\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        \n",
    "#         pool_out_size=(hid_sizes[-1]-2)/2\n",
    "        self.output_layer=nn.Linear(hid_sizes[-1], out_size)\n",
    "    def evaluate(self, x):\n",
    "        with torch.no_grad():\n",
    "            return self.forward(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= fl.relu(self.input_layer(x))\n",
    "        # Pass through fully connected layers with ReLU activation\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            x = fl.relu(self.hidden_layers[i](x))\n",
    "#             if i==len(self.hidden_sizes)-1:\n",
    "#                 x = x.unsqueeze(0).unsqueeze(0)\n",
    "#                 x = self.pool(x)\n",
    "            \n",
    "#             x = fl.relu(self.fc2(x))\n",
    "\n",
    "        # Apply pooling layer\n",
    "#         print('input to pool: ',x.size())\n",
    "        \n",
    "        x=torch.max(x,dim=-2).values\n",
    "        # Pass through final fully connected layer for output\n",
    "        x = fl.relu(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fff31219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(device, tunnel_backlogs, packets_in_flight, look_back):\n",
    "    # class to split data, scale data, and rescale data\n",
    "    data_processer = data_processer_functions.DataProcessor()\n",
    "\n",
    "    # create time series sequences with given look_back hyperparameter\n",
    "    x_all = data_processer.create_sequences(torch.Tensor(packets_in_flight), look_back).to(device)\n",
    "    y_all = torch.Tensor(tunnel_backlogs).to(device)\n",
    "    \n",
    "    # feature engineering\n",
    "    x_transformed = data_processer.feature_transform(device, x_all)\n",
    "\n",
    "    # split into train and test\n",
    "    x_train_unscaled, y_train_unscaled, x_test_unscaled, y_test_unscaled = data_processer.split_train_test(x_transformed, y_all)\n",
    "\n",
    "    # rescale to [0,1]\n",
    "    x_train, y_train = data_processer.scale_train(x_train_unscaled, y_train_unscaled, is_x_sequenced = True)\n",
    "    x_test, y_test = data_processer.scale_test(x_test_unscaled, y_test_unscaled)\n",
    "\n",
    "    return data_processer, x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23a780a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_and_train_model(device, hidden_sizes, learning_rate, num_epochs, x_train, y_train):\n",
    "    # seeds  for reproducibility\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # define model with given hidden_sizes\n",
    "    input_size  = x_train.shape[-1]\n",
    "    output_size = y_train.shape[-1]\n",
    "    dnn_mod = DNN_pool(input_size, hidden_sizes, output_size).to(device)\n",
    "    print(dnn_mod)\n",
    "\n",
    "    # define tools for training with given hyperparameters\n",
    "    trainer = model_trainer_class.modelTrainer(criterion = nn.MSELoss(), device = device)\n",
    "    optimizer = torch.optim.Adam(dnn_mod.parameters(), lr = learning_rate)\n",
    "\n",
    "    # training loop\n",
    "    pbar = trange(num_epochs)\n",
    "    for epoch in pbar:\n",
    "        # perform a training epoch using full x_train dataset\n",
    "        # (future): if x_train is too large, break into batches, and use batches x_batch instead \n",
    "        loss_value = trainer.batch_step(dnn_mod, x_train, y_train, optimizer) \n",
    "        pbar.set_postfix({'epoch': f'{epoch+1}/{num_epochs}', 'loss': loss_value, 'look_back': x_train.shape[1], 'hidden_sizes': str(hidden_sizes), 'learning_rate': learning_rate})\n",
    "\n",
    "    return dnn_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff16a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_and_get_performance(device, tunnel_backlogs, packets_in_flight, look_back, hidden_sizes, learning_rate, num_epochs):\n",
    "    # prepare data\n",
    "    data_processer, x_train, y_train, x_test, y_test = prepare_data(device, tunnel_backlogs, packets_in_flight, look_back)\n",
    "\n",
    "    # define and train model\n",
    "    dnn_mod = define_and_train_model(device, hidden_sizes, learning_rate, num_epochs, x_train, y_train)\n",
    "\n",
    "    # get performance metrics\n",
    "    train_error_rates, _, _ = get_error_rates(dnn_mod, x_train, y_train, data_processer)\n",
    "    test_error_rates, _, _ = get_error_rates(dnn_mod, x_test, y_test, data_processer)\n",
    "    error_metrics = {'train': train_error_rates, 'test': test_error_rates}\n",
    "    \n",
    "    return dnn_mod, error_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8426589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_rates(lstm_predictor, x, y, data_processer):\n",
    "    # get predictions\n",
    "    lstm_predictor.eval()\n",
    "    y_pred = lstm_predictor.evaluate(x)\n",
    "\n",
    "    # rescale back to queue sizes\n",
    "    _, y_pred_unscaled = data_processer.inverse_scale(x_scaled=0, y_scaled=y_pred)\n",
    "    _, y_unscaled = data_processer.inverse_scale(x_scaled=0, y_scaled=y)\n",
    "    y_pred_unscaled = np.round(y_pred_unscaled.cpu().numpy())\n",
    "    y_unscaled = y_unscaled.cpu().numpy()\n",
    "\n",
    "    # calculate root mean squared error, and max absolute percentage error\n",
    "    rmse = mean_squared_error(y_unscaled, y_pred_unscaled, squared = False) # rms = np.sqrt(np.mean((Y_predicted-Y)**2))\n",
    "    mape = mean_absolute_percentage_error(y_unscaled[y_unscaled>0], y_pred_unscaled[y_unscaled>0])*100 # mape = 100*np.mean(np.abs(y_pred_unscaled-y_unscaled)[y_unscaled>0]/y_unscaled[y_unscaled>0])\n",
    "\n",
    "    error_rates = {'rmse': rmse,\n",
    "                   'mape': mape}\n",
    "\n",
    "    return error_rates, y_unscaled, y_pred_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e61c5b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden sizes is : [30, 20, 10, 20]\n",
      "DNN_pool(\n",
      "  (input_layer): Linear(in_features=5, out_features=30, bias=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=20, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=20, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:02<00:00, 190.03it/s, epoch=400/400, loss=0.00158, look_back=1, hidden_sizes=[30, 20, 10, 20], learning_rate=0.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden sizes is : [30, 20, 10, 20]\n",
      "DNN_pool(\n",
      "  (input_layer): Linear(in_features=5, out_features=30, bias=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=20, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=20, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:01<00:00, 214.03it/s, epoch=400/400, loss=0.0016, look_back=2, hidden_sizes=[30, 20, 10, 20], learning_rate=0.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden sizes is : [30, 20, 10, 20]\n",
      "DNN_pool(\n",
      "  (input_layer): Linear(in_features=5, out_features=30, bias=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=20, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=20, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:02<00:00, 199.70it/s, epoch=400/400, loss=0.00153, look_back=3, hidden_sizes=[30, 20, 10, 20], learning_rate=0.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden sizes is : [30, 20, 10, 20]\n",
      "DNN_pool(\n",
      "  (input_layer): Linear(in_features=5, out_features=30, bias=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=20, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=20, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:02<00:00, 170.33it/s, epoch=400/400, loss=0.00184, look_back=4, hidden_sizes=[30, 20, 10, 20], learning_rate=0.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden sizes is : [30, 20, 10, 20]\n",
      "DNN_pool(\n",
      "  (input_layer): Linear(in_features=5, out_features=30, bias=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=20, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=20, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:02<00:00, 147.33it/s, epoch=400/400, loss=0.0016, look_back=5, hidden_sizes=[30, 20, 10, 20], learning_rate=0.01] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden sizes is : [30, 20, 10, 20]\n",
      "DNN_pool(\n",
      "  (input_layer): Linear(in_features=5, out_features=30, bias=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=20, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=20, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:04<00:00, 90.45it/s, epoch=400/400, loss=0.00183, look_back=10, hidden_sizes=[30, 20, 10, 20], learning_rate=0.01]\n"
     ]
    }
   ],
   "source": [
    "# reload(data_processer_functions)\n",
    "# reload(lstm_model_class)\n",
    "# reload(model_trainer_class)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# create hyperparamer lists to explore\n",
    "look_back_list = [1,2,3,4,5,10]\n",
    "hidden_sizes_list = [[30,20,10,20]]\n",
    "learning_rate_list = [1e-2] #[5e-2, 1e-2, 5e-3]\n",
    "num_epochs = [400]\n",
    "saved_models = []\n",
    "# model, error_metrics = train_model_and_get_performance(device, tunnel_backlogs, packets_in_flight, 10, [10,20,10,5], 1e-2, num_epochs)\n",
    "\n",
    "for look_back in look_back_list:\n",
    "    for hidden_sizes in hidden_sizes_list:\n",
    "        for learning_rate in learning_rate_list:\n",
    "            for ep in num_epochs:\n",
    "            # train and test the model\n",
    "                model, error_metrics = train_model_and_get_performance(device, tunnel_backlogs, packets_in_flight, look_back, hidden_sizes, learning_rate, ep)\n",
    "\n",
    "                # save trained model\n",
    "                saved_models.append({\n",
    "                            'model' : model,\n",
    "                            'look_back': look_back,\n",
    "                            'learning_rate': learning_rate,\n",
    "                            'hidden_size' : hidden_sizes,\n",
    "                            'number_of_epochs': ep,\n",
    "                            'error_metrics': error_metrics\n",
    "                        })\n",
    "                \n",
    "# sort models according to best test rmse error\n",
    "saved_models.sort(key = lambda x: (x['error_metrics']['test']['rmse'], x['error_metrics']['test']['mape'])) # (future): technically should have validation data and sort according to that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fedff5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'rmse': 6.2829084, 'mape': 9.593518823385239},\n",
       " 'test': {'rmse': 5.966451, 'mape': 9.480760991573334}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4745012e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, {'rmse': 4.9868903, 'mape': 7.797548174858093}), (3, {'rmse': 5.0521026, 'mape': 7.959417998790741}), (2, {'rmse': 5.160409, 'mape': 8.141030371189117}), (5, {'rmse': 5.184597, 'mape': 8.182130008935928}), (4, {'rmse': 5.9346933, 'mape': 9.444775432348251}), (10, {'rmse': 5.966451, 'mape': 9.480760991573334})]\n"
     ]
    }
   ],
   "source": [
    "print([(sf['look_back'],sf['error_metrics']['test']) for sf in saved_models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f50842d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min is:  {'model': DNN_pool(\n",
      "  (input_layer): Linear(in_features=5, out_features=30, bias=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=30, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=20, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=20, out_features=2, bias=True)\n",
      "), 'look_back': 1, 'learning_rate': 0.01, 'hidden_size': [30, 20, 10, 20], 'number_of_epochs': 400, 'error_metrics': {'train': {'rmse': 5.521661, 'mape': 8.28055664896965}, 'test': {'rmse': 4.9868903, 'mape': 7.797548174858093}}}\n"
     ]
    }
   ],
   "source": [
    "print('min is: ',min(saved_models,key=lambda x: x['error_metrics']['test']['rmse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f19a6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.3761"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5.69**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b2178de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split in module sklearn.model_selection._split:\n",
      "\n",
      "train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
      "    Split arrays or matrices into random train and test subsets.\n",
      "    \n",
      "    Quick utility that wraps input validation,\n",
      "    ``next(ShuffleSplit().split(X, y))``, and application to input data\n",
      "    into a single call for splitting (and optionally subsampling) data into a\n",
      "    one-liner.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *arrays : sequence of indexables with same length / shape[0]\n",
      "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "        matrices or pandas dataframes.\n",
      "    \n",
      "    test_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "        of the dataset to include in the test split. If int, represents the\n",
      "        absolute number of test samples. If None, the value is set to the\n",
      "        complement of the train size. If ``train_size`` is also None, it will\n",
      "        be set to 0.25.\n",
      "    \n",
      "    train_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the train split. If\n",
      "        int, represents the absolute number of train samples. If None,\n",
      "        the value is automatically set to the complement of the test size.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Controls the shuffling applied to the data before applying the split.\n",
      "        Pass an int for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    shuffle : bool, default=True\n",
      "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "        then stratify must be None.\n",
      "    \n",
      "    stratify : array-like, default=None\n",
      "        If not None, data is split in a stratified fashion, using this as\n",
      "        the class labels.\n",
      "        Read more in the :ref:`User Guide <stratification>`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    splitting : list, length=2 * len(arrays)\n",
      "        List containing train-test split of inputs.\n",
      "    \n",
      "        .. versionadded:: 0.16\n",
      "            If the input is sparse, the output will be a\n",
      "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "            input type.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.model_selection import train_test_split\n",
      "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "    >>> X\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5],\n",
      "           [6, 7],\n",
      "           [8, 9]])\n",
      "    >>> list(y)\n",
      "    [0, 1, 2, 3, 4]\n",
      "    \n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ...     X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train\n",
      "    array([[4, 5],\n",
      "           [0, 1],\n",
      "           [6, 7]])\n",
      "    >>> y_train\n",
      "    [2, 0, 3]\n",
      "    >>> X_test\n",
      "    array([[2, 3],\n",
      "           [8, 9]])\n",
      "    >>> y_test\n",
      "    [1, 4]\n",
      "    \n",
      "    >>> train_test_split(y, shuffle=False)\n",
      "    [[0, 1, 2], [3, 4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff746ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
