{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11cf7bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import utils.network as network_class\n",
    "# import utils.lstm_model as lstm_model_class\n",
    "import utils.model_trainer as model_trainer_class\n",
    "import utils.data_processer as data_processer_functions\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fl\n",
    "\n",
    "from importlib import reload\n",
    "from tqdm import trange\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bffe637d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2080 Ti\n",
      "CUDA Version 12.1\n",
      "PyTorch Version 2.1.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "print('CUDA Version', torch.version.cuda)\n",
    "print('PyTorch Version', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c87f3",
   "metadata": {},
   "source": [
    "# Defining the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03b55891",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 5\n",
    "num_edges = 5\n",
    "\n",
    "adjacency_nodexedge = np.zeros([num_nodes, num_edges]) # node x edge adjacency matrix of the network (with all tunnels)\n",
    "adjacency_nodexedge = np.array([[-1,0,0,0,0], # -1 => node is edge's source\n",
    "                                [1,-1,-1,0,0], # 1 => node is edge's destination\n",
    "                                [0,1,0,-1,0],\n",
    "                                [0,0,1,1,-1],\n",
    "                                [0,0,0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8110eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tunnel level parameters\n",
    "num_tunnels = 2\n",
    "\n",
    "is_edge_in_tunnel      = np.zeros([num_tunnels, num_edges, ])\n",
    "external_arrival_rates = np.zeros([num_tunnels, num_nodes, ])\n",
    "overlay_service_rates  = np.zeros([num_tunnels, num_edges, ])\n",
    "underlay_service_rates = np.zeros([num_tunnels, num_edges, ])\n",
    "\n",
    "# tunnel 0\n",
    "is_edge_in_tunnel[0,:]      = np.array([1,1,0,1,1]) # 1 => edge is a part of the tunnel, 0 => otherwise\n",
    "external_arrival_rates[0,:] = np.array([0.9,0,0,0,0])\n",
    "overlay_service_rates[0,:]  = np.array([1,0,0,0,0])\n",
    "underlay_service_rates[0,:] = np.array([0,1,0,1,1])\n",
    "\n",
    "# tunnel 1\n",
    "is_edge_in_tunnel[1,:]      = np.array([1,0,1,0,1]) # 1 => edge is a part of the tunnel, 0 => otherwise\n",
    "external_arrival_rates[1,:] = np.array([0.9,0,0,0,0])\n",
    "overlay_service_rates[1,:]  = np.array([1,0,0,0,0])\n",
    "underlay_service_rates[1,:] = np.array([0,0,1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "362cee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get network class\n",
    "reload(network_class)\n",
    "my_network = network_class.Network(\n",
    "    num_tunnels,\n",
    "    num_nodes,\n",
    "    num_edges,\n",
    "    adjacency_nodexedge,\n",
    "    is_edge_in_tunnel, \n",
    "    underlay_service_rates,\n",
    "    external_arrival_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3321ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate network\n",
    "packets_in_flight, tunnel_backlogs = my_network.simulate(overlay_service_rates, total_time = 100000, custom_seed = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aef40c",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "254df219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class DNN_pool(nn.Module):\n",
    "    def __init__(self,in_size,hid_sizes,out_size):\n",
    "        super(DNN_pool, self).__init__()\n",
    "        self.input_size=in_size\n",
    "        self.output_size=out_size\n",
    "        self.hidden_sizes=hid_sizes\n",
    "        print('hidden sizes is :',hid_sizes)\n",
    "        \n",
    "        # Fully connected hidden layers\n",
    "        self.input_layer = nn.Linear(in_size, hid_sizes[0])  \n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(hid_sizes[i], hid_sizes[i+1]) \n",
    "                                            for i in range(len(hid_sizes) - 1)])\n",
    "        \n",
    "#         self.fc2 = nn.Linear(128, 64)\n",
    "#         self.fc3 = nn.Linear(64, out_size)\n",
    "\n",
    "        # Max pooling layer\n",
    "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        \n",
    "#         pool_out_size=(hid_sizes[-1]-2)/2\n",
    "        self.output_layer=nn.Linear(hid_sizes[-1], out_size)\n",
    "    def evaluate(self, x):\n",
    "        with torch.no_grad():\n",
    "            return self.forward(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x= fl.relu(self.input_layer(x))\n",
    "        # Pass through fully connected layers with ReLU activation\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            x = fl.relu(self.hidden_layers[i](x))\n",
    "#             if i==len(self.hidden_sizes)-1:\n",
    "#                 x = x.unsqueeze(0).unsqueeze(0)\n",
    "#                 x = self.pool(x)\n",
    "            \n",
    "#             x = fl.relu(self.fc2(x))\n",
    "\n",
    "        # Apply pooling layer\n",
    "#         print('input to pool: ',x.size())\n",
    "        \n",
    "        x=torch.max(x,dim=-2).values\n",
    "        # Pass through final fully connected layer for output\n",
    "        x = fl.relu(self.output_layer(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff31219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(device, tunnel_backlogs, packets_in_flight, look_back):\n",
    "    # class to split data, scale data, and rescale data\n",
    "    data_processer = data_processer_functions.DataProcessor()\n",
    "\n",
    "    # create time series sequences with given look_back hyperparameter\n",
    "    x_all = data_processer.create_sequences(torch.Tensor(packets_in_flight), look_back).to(device)\n",
    "    y_all = torch.Tensor(tunnel_backlogs).to(device)\n",
    "    \n",
    "    # feature engineering\n",
    "    x_transformed = data_processer.feature_transform(device, x_all)\n",
    "\n",
    "    # split into train and test\n",
    "    x_train_unscaled, y_train_unscaled, x_test_unscaled, y_test_unscaled = data_processer.split_train_test(x_transformed, y_all)\n",
    "\n",
    "    # rescale to [0,1]\n",
    "    x_train, y_train = data_processer.scale_train(x_train_unscaled, y_train_unscaled, is_x_sequenced = True)\n",
    "    x_test, y_test = data_processer.scale_test(x_test_unscaled, y_test_unscaled)\n",
    "\n",
    "    return data_processer, x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23a780a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_and_train_model(device, hidden_sizes, learning_rate, num_epochs, x_train, y_train):\n",
    "    # seeds  for reproducibility\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # define model with given hidden_sizes\n",
    "    input_size  = x_train.shape[-1]\n",
    "    output_size = y_train.shape[-1]\n",
    "    dnn_mod = DNN_pool(input_size, hidden_sizes, output_size).to(device)\n",
    "    print(dnn_mod)\n",
    "\n",
    "    # define tools for training with given hyperparameters\n",
    "    trainer = model_trainer_class.modelTrainer(criterion = nn.MSELoss(), device = device)\n",
    "    optimizer = torch.optim.Adam(dnn_mod.parameters(), lr = learning_rate)\n",
    "\n",
    "    # training loop\n",
    "    pbar = trange(num_epochs)\n",
    "    for epoch in pbar:\n",
    "        # perform a training epoch using full x_train dataset\n",
    "        # (future): if x_train is too large, break into batches, and use batches x_batch instead \n",
    "        loss_value = trainer.batch_step(dnn_mod, x_train, y_train, optimizer) \n",
    "        pbar.set_postfix({'epoch': f'{epoch+1}/{num_epochs}', 'loss': loss_value, 'look_back': x_train.shape[1], 'hidden_sizes': str(hidden_sizes), 'learning_rate': learning_rate})\n",
    "\n",
    "    return dnn_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff16a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_and_get_performance(device, tunnel_backlogs, packets_in_flight, look_back, hidden_sizes, learning_rate, num_epochs):\n",
    "    # prepare data\n",
    "    data_processer, x_train, y_train, x_test, y_test = prepare_data(device, tunnel_backlogs, packets_in_flight, look_back)\n",
    "\n",
    "    # define and train model\n",
    "    dnn_mod = define_and_train_model(device, hidden_sizes, learning_rate, num_epochs, x_train, y_train)\n",
    "\n",
    "    # get performance metrics\n",
    "    train_error_rates, _, _ = get_error_rates(dnn_mod, x_train, y_train, data_processer)\n",
    "    test_error_rates, _, _ = get_error_rates(dnn_mod, x_test, y_test, data_processer)\n",
    "    error_metrics = {'train': train_error_rates, 'test': test_error_rates}\n",
    "    \n",
    "    return dnn_mod, error_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8426589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_rates(lstm_predictor, x, y, data_processer):\n",
    "    # get predictions\n",
    "    lstm_predictor.eval()\n",
    "    y_pred = lstm_predictor.evaluate(x)\n",
    "\n",
    "    # rescale back to queue sizes\n",
    "    _, y_pred_unscaled = data_processer.inverse_scale(x_scaled=0, y_scaled=y_pred)\n",
    "    _, y_unscaled = data_processer.inverse_scale(x_scaled=0, y_scaled=y)\n",
    "    y_pred_unscaled = np.round(y_pred_unscaled.cpu().numpy())\n",
    "    y_unscaled = y_unscaled.cpu().numpy()\n",
    "\n",
    "    # calculate root mean squared error, and max absolute percentage error\n",
    "    rmse = mean_squared_error(y_unscaled, y_pred_unscaled, squared = False) # rms = np.sqrt(np.mean((Y_predicted-Y)**2))\n",
    "    mape = mean_absolute_percentage_error(y_unscaled[y_unscaled>0], y_pred_unscaled[y_unscaled>0])*100 # mape = 100*np.mean(np.abs(y_pred_unscaled-y_unscaled)[y_unscaled>0]/y_unscaled[y_unscaled>0])\n",
    "\n",
    "    error_rates = {'rmse': rmse,\n",
    "                   'mape': mape}\n",
    "\n",
    "    return error_rates, y_unscaled, y_pred_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e61c5b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden sizes is : [10, 20, 10, 5]\n",
      "DNN_pool(\n",
      "  (input_layer): Linear(in_features=5, out_features=10, bias=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=5, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=5, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆ| 250/250 [00:02<00:00, 103.45it/s, epoch=250/250, loss=0.175, look_back=10, hidden_sizes=[10, 20, 10, 5], learni\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden sizes is : [5, 5, 5]\n",
      "DNN_pool(\n",
      "  (input_layer): Linear(in_features=5, out_features=5, bias=True)\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=5, out_features=5, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=5, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆ| 250/250 [00:01<00:00, 170.47it/s, epoch=250/250, loss=0.0023, look_back=10, hidden_sizes=[5, 5, 5], learning_ra\n"
     ]
    }
   ],
   "source": [
    "# reload(data_processer_functions)\n",
    "# reload(lstm_model_class)\n",
    "# reload(model_trainer_class)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# create hyperparamer lists to explore\n",
    "look_back_list = [10]\n",
    "hidden_sizes_list = [[10,20,10,5], [5,5,5]]\n",
    "learning_rate_list = [1e-2] #[5e-2, 1e-2, 5e-3]\n",
    "num_epochs = 250\n",
    "saved_models = []\n",
    "# model, error_metrics = train_model_and_get_performance(device, tunnel_backlogs, packets_in_flight, 10, [10,20,10,5], 1e-2, num_epochs)\n",
    "\n",
    "for look_back in look_back_list:\n",
    "    for hidden_sizes in hidden_sizes_list:\n",
    "        for learning_rate in learning_rate_list:\n",
    "            # train and test the model\n",
    "            model, error_metrics = train_model_and_get_performance(device, tunnel_backlogs, packets_in_flight, look_back, hidden_sizes, learning_rate, num_epochs)\n",
    "\n",
    "            # save trained model\n",
    "            saved_models.append({\n",
    "                        'model' : model,\n",
    "                        'look_back': look_back,\n",
    "                        'learning_rate': learning_rate,\n",
    "                        'hidden_size' : hidden_sizes,\n",
    "                        'error_metrics': error_metrics\n",
    "                    })\n",
    "            \n",
    "# sort models according to best test rmse error\n",
    "saved_models.sort(key = lambda x: (x['error_metrics']['test']['rmse'], x['error_metrics']['test']['mape'])) # (future): technically should have validation data and sort according to that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fedff5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'rmse': 7.362185, 'mape': 11.26992553472519},\n",
       " 'test': {'rmse': 7.0383043, 'mape': 11.253330111503601}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b51b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
